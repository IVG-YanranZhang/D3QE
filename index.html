<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title"
    content="[ICCV 2025] D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection">
  <meta name="description"
    content="D³QE is a novel framework for detecting autoregressive-generated images by exploiting the distinctive patterns and frequency bias in the discrete latent codebook, achieving superior accuracy and generalization.">
  <meta name="keywords"
    content="Autoregressive Image Detection, Codebook Distribution Bias, Quantization Error, D3QE, ARForensics, ICCV 2025, computer vision, AI, machine learning">
  <meta name="author"
    content="Yanran Zhang, Bingyao Yu, Yu Zheng, Wenzhao Zheng, Yueqi Duan, Lei Chen, Jie Zhou, Jiwen Lu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Tsinghua University">
  <meta property="og:title"
    content="D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection">
  <meta property="og:description"
    content="D³QE is a novel framework for detecting autoregressive-generated images by exploiting the distinctive patterns and frequency bias in the discrete latent codebook, achieving superior accuracy and generalization.">
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="D³QE - Research Preview">
  <meta property="article:published_time" content="2025-10-01T00:00:00.000Z">
  <meta property="article:author" content="Yanran Zhang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Autoregressive Image Detection">
  <meta property="article:tag" content="Quantization Error">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title"
    content="D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection">
  <meta name="twitter:description"
    content="D³QE is a novel framework for detecting autoregressive-generated images by exploiting the distinctive patterns and frequency bias in the discrete latent codebook, achieving superior accuracy and generalization.">
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="D³QE - Research Preview">

  <meta name="citation_title"
    content="D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection">
  <meta name="citation_author" content="Zhang, Yanran">
  <meta name="citation_author" content="Yu, Bingyao">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="ICCV">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/ftnfczwwqrhcpyzqbbqrfxbrdnpzcwyg.pdf">

  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>D³QE - Yanran Zhang, Bingyao Yu et al. | Academic Research</title>

  <link rel="icon" type="image/x-icon" href="static/images/d3qe.ico">
  <link rel="apple-touch-icon" href="static/images/d3qe.ico">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection",
    "description": "D³QE is a novel framework for detecting autoregressive-generated images by exploiting the distinctive patterns and frequency bias in the discrete latent codebook, achieving superior accuracy and generalization.",
    "author": [
      {
        "@type": "Person",
        "name": "Yanran Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "Tsinghua University"
        }
      },
      {
        "@type": "Person",
        "name": "Bingyao Yu",
        "affiliation": {
          "@type": "Organization",
          "name": "Tsinghua University"
        }
      },
      {
        "@type": "Person",
        "name": "Yu Zheng",
        "affiliation": {
          "@type": "Organization",
          "name": "Tsinghua University"
        }
      }
      // Add more authors as needed
    ],
    "datePublished": "2025-10-01",
    "publisher": {
      "@type": "Organization",
      "name": "ICCV"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["Autoregressive Image Detection", "Codebook Distribution Bias", "Quantization Error", "D3QE", "ARForensics", "ICCV 2025", "machine learning", "computer vision"],
    "abstract": "The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D³QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D³QE across different AR models, with robustness to real-world perturbations. Code is available at https://github.com/Zhangyr2022/D3QE.",
    "citation": "@article{Zhang2025D3QE,\n  title={D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection},\n  author={Yanran Zhang and Bingyao Yu and Yu Zheng and Wenzhao Zheng and Yueqi Duan and Lei Chen and Jie Zhou and Jiwen Lu},\n  journal={ICCV},\n  year={2025},\n  url={https://YOUR-DOMAIN.com/YOUR-PROJECT-PAGE}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Image Forensics"
      },
      {
        "@type": "Thing", 
        "name": "Autoregressive Models"
      }
    ]
  }
  </script>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Tsinghua University",
    "url": "https://www.tsinghua.edu.cn/",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>

<body>


  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Yanran Zhang</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://ivg-yanranzhang.github.io/UniPre3D/" class="work-item" target="_blank">
          <div class="work-info">
            <h5>UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting</h5>
            <p>UniPre3D is the first unified pre-training method for 3D point clouds that effectively handles both
              object- and scene-level data through cross-modal Gaussian splatting.</p>
            <span class="work-venue">CVPR 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">D³QE: Learning Discrete Distribution Discrepancy-aware
                Quantization Error for Autoregressive-Generated Image Detection</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="mailto:zhangyr21@mails.tsinghua.edu.cn" target="_blank">Yanran Zhang</a><sup>*</sup>¹,</span>
                <span class="author-block">
                  <a href="mailto:yuby@tsinghua.edu.cn" target="_blank">Bingyao Yu</a><sup>*,†</sup>¹,</span>
                <span class="author-block">
                  <a href="mailto:yu-zheng@tsinghua.edu.cn" target="_blank">Yu Zheng</a>¹</span>
                <span class="author-block">
                  <a href="mailto:wenzhao.zheng@outlook.com" target="_blank">Wenzhao Zheng</a>¹</span>
                <span class="author-block">
                  <a href="mailto:duanyueqi@tsinghua.edu.cn" target="_blank">Yueqi Duan</a>²</span>
                <span class="author-block">
                  <a href="mailto:leichenthu@tsinghua.edu.cn" target="_blank">Lei Chen</a>¹</span>
                <span class="author-block">
                  <a href="mailto:jzhou@tsinghua.edu.cn" target="_blank">Jie Zhou</a>¹</span>
                <span class="author-block">
                  <a href="mailto:lujiwen@tsinghua.edu.cn" target="_blank">Jiwen Lu</a><sup>†</sup>¹</span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">¹ Department of Automation, Tsinghua University, China<br>² Department of
                  Electronic Engineering, Tsinghua University, China<br>ICCV 2025</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution, <sup>†</sup>Corresponding
                    Author</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="static/pdfs/ftnfczwwqrhcpyzqbbqrfxbrdnpzcwyg.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/Zhangyr2022/D3QE" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <!-- <div class="is-centered">
            <img src="static/images/d3qe_teaser.png" alt=" Discrete Distribution Discrepancy " loading="lazy" />
            <img src="static/images/d3qe_vis.png" alt=" Discrete Distribution Discrepancy " loading="lazy" />
          </div> -->
          <div class="columns is-centered is-vcentered">
            <div class="column is-half has-text-centered">
              <img src="static/images/d3qe_teaser.png" alt="D³QE Pipeline Diagram" loading="lazy"
                style="width: 90%; display: inline-block;">
            </div>
            <div class="column is-half has-text-centered">
              <img src="static/images/d3qe_vis.png" alt="Codebook Activation Difference Visualization" loading="lazy"
                style="width: 90%; display: inline-block;">
            </div>
          </div>
          <h2 class="subtitle has-text-centered">
            Our key insight is the stark distribution discrepancy in the discrete latent space of autoregressive(AR)
            models.
            Token distribution plots reveal the long-tail token usage in real data versus the concentrated,
            high-frequency bias in fakes. This is confirmed by codebook activation heatmaps, which show a balanced
            pattern for real images but polarized hotspots for fakes—the core artifact for our detection method.
          </h2>
        </div>
      </div>
    </section>
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The emergence of visual autoregressive (AR) models has revolutionized image generation while
                presenting new challenges for synthetic image detection. Unlike previous GAN or
                diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both
                marked improvements in image synthesis quality and unique characteristics in their vector-quantized
                representations. In this paper, we propose to leverage <b>Discrete Distribution
                  Discrepancy-aware Quantization Error</b> <b>D³QE</b> for autoregressive-generated image
                detection that exploits the distinctive patterns and the frequency distribution bias of the codebook
                existing in real and fake images. We introduce a <b>discrete distribution
                  discrepancy-aware transformer</b> that integrates dynamic codebook frequency statistics into its
                attention mechanism, fusing semantic features and quantization error latent. To evaluate our
                method, we construct a comprehensive dataset termed <b>ARForensics</b> covering 7 mainstream visual AR
                models. Experiments demonstrate superior detection accuracy and strong
                generalization of D³QE across different AR models, with robustness to real-world
                perturbations.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <hr>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Introduction & Contributions</h2>
            <img src="static/images/d3qe_pipe.png" alt="D3QE Pipeline" loading="lazy" />

            <div class="content has-text-justified">
              <p>
                Autoregressive (AR) models introduce new detection challenges: forgery artifacts are encoded in the
                <b>discrete latent space</b> (via <b>discrete token prediction</b>), not in typical pixel noise,
                rendering conventional detectors ineffective. Our key insight is the stark <b>Discrete Distribution
                  Discrepancy</b> (D³D) between real and AR-generated images. Real data exhibits a <b>long-tail token
                  distribution</b>, while fake images show probability mass concentrated in high-frequency regions,
                indicating polarized codebook usage.
              </p>

              <h4 class="title is-5">Our Main Contributions:</h4>
              <ul>
                <li><b>D³QE Framework:</b> We propose D³QE, a novel framework that systematically analyzes and leverages
                  both the <b>codebook distribution bias</b> and the <b>quantization error</b> inherent in the AR
                  generation process.</li>
                <li><b>D³AT Transformer:</b> We introduce a specialized transformer with <b>Discrepancy-Aware
                    Self-Attention</b> (D³ASA). This mechanism integrates dynamic codebook frequency statistics ($\Delta
                  D$) to effectively fuse the quantization error latent with robust semantic features.</li>
                <li><b>ARForensics Benchmark:</b> We establish the first comprehensive dataset, <b>ARForensics</b>, for
                  AR-generated image detection, covering <b>7 mainstream visual AR models</b> to rigorously test
                  generalization and robustness.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>
    <hr>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Methodology: The D³QE Pipeline</h2>
            <div class="content has-text-justified">
              <p>
                The D³QE framework fuses local discrete artifacts with global semantic features through four key
                components:
              </p>

              <ol>
                <li><b>Quantization Error Representation:</b> A frozen <b>VQVAE Encoder</b> extracts the error between
                  the continuous latent map z and its discrete representation z_q.</li>
                <li><b>Discrete Distribution Statistics:</b> Computes the <b>discrete distribution discrepancy</b> (ΔD)
                  from real vs. fake token usage statistics.</li>
                <li><b>D³AT Transformer:</b> Its core <b>Discrepancy-Aware Self-Attention</b> (D³ASA) module processes
                  the quantization error, guided by global ΔD.</li>
                <li><b>Semantic Feature Fusion:</b> A frozen <b>CLIP</b> extracts semantic features, which are fused
                  with the D³AT's output for final classification.</li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </section>

    <hr>


    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Dataset Visualization</h2>
          <div class="content has-text-centered">
            <p>
              The <b>ARForensics</b> dataset contains samples from 7 mainstream AR models ( <b>LlamaGen, VAR,
                Infinity,
                Janus-Pro, RAR, Switti, Open-MAGVIT2</b>), serving as a robust visual benchmark for testing the
              generalization of ai-generated image detection models.
            </p>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video">
                <iframe src="static/videos/data.mp4" frameborder="0" allow="encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <hr>
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Experiments</h2>
            <div class="content has-text-justified">
              <p>
                We evaluate $\text{D}^3\text{QE}$ extensively on our proposed **ARForensics** benchmark, demonstrating
                its superior performance in both intra-model testing and cross-model generalization.
              </p>

              <h4 class="title is-5">Table 1: Intra-Model Detection Performance (Accuracy / A.P.)</h4>
              <p>
                This table summarizes the performance of various methods when trained and tested on the same model from
                the ARForensics dataset. **$\text{D}^3\text{QE}$ demonstrates superior overall performance (Mean Acc. /
                A.P.)** by effectively leveraging the discrete distribution artifacts.
              </p>

              <table class="table is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>LlamaGen<br>Acc. / A.P.</th>
                    <th>VAR<br>Acc. / A.P.</th>
                    <th>Infinity<br>Acc. / A.P.</th>
                    <th>Janus-Pro<br>Acc. / A.P.</th>
                    <th>RAR<br>Acc. / A.P.</th>
                    <th>Switti<br>Acc. / A.P.</th>
                    <th>Open-MAGVIT2<br>Acc. / A.P.</th>
                    <th>Mean<br>Acc. / A.P.</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>CNNSpot [55]</td>
                    <td>99.94 / 99.94</td>
                    <td>50.26 / 70.53</td>
                    <td>50.87 / 78.06</td>
                    <td>95.70 / 99.95</td>
                    <td>50.80 / 61.67</td>
                    <td>93.91 / 56.58</td>
                    <td>50.12 / 57.39</td>
                    <td>64.90 / 80.21</td>
                  </tr>
                  <tr>
                    <td>FreDect [11]</td>
                    <td>99.80 / 100.00</td>
                    <td>52.88 / 88.18</td>
                    <td>50.17 / 60.13</td>
                    <td>88.94 / 99.54</td>
                    <td>52.52 / 83.31</td>
                    <td>59.01 / 50.04</td>
                    <td>57.09 / 86.53</td>
                    <td>64.49 / 82.39</td>
                  </tr>
                  <tr>
                    <td>Gram-Net [25]</td>
                    <td>99.57 / 99.98</td>
                    <td>55.04 / 84.57</td>
                    <td>52.38 / 76.80</td>
                    <td>74.48 / 97.33</td>
                    <td>49.95 / 52.72</td>
                    <td>57.74 / 88.66</td>
                    <td>53.72 / 50.08</td>
                    <td>62.75 / 79.11</td>
                  </tr>
                  <tr>
                    <td>LNP [23]</td>
                    <td>99.48 / 99.99</td>
                    <td>49.64 / 55.42</td>
                    <td>49.76 / 49.94</td>
                    <td>99.53 / 99.98</td>
                    <td>49.69 / 55.61</td>
                    <td>70.28 / 95.83</td>
                    <td>54.67 / 76.81</td>
                    <td>67.58 / 82.74</td>
                  </tr>
                </tbody>
              </table>

              <h4 class="title is-5">Table 2: Cross-Model Generalization</h4>
              <p>
                This table presents the results of cross-model testing, where the model is trained on one AR model
                (e.g., LlamaGen) and tested on an unseen model (e.g., VAR). $\text{D}^3\text{QE}$ demonstrates
                exceptional generalization ability, significantly outperforming competitors, confirming that the
                extracted discrete distribution discrepancy is a general artifact of AR models. **(Full table in the
                paper)**
              </p>

              <h4 class="title is-5">Table 3: Robustness to Post-Processing Perturbations</h4>
              <p>
                This table evaluates the model's robustness under common real-world corruptions, such as JPEG
                compression and Gaussian blurring. $\text{D}^3\text{QE}$ maintains high detection accuracy across all
                perturbation levels, validating its resilience against image manipulation, which is critical for
                practical applications. **(Full table in the paper)**
              </p>

              <h4 class="title is-5">Table 4: Ablation Study on D³QE Components</h4>
              <p>
                The ablation study confirms the contribution of each proposed module: the Quantization Error feature,
                the Distribution Discrepancy ($\Delta D$) feature, and the $\text{D}^3\text{ASA}$ mechanism. Results
                show that combining both $\Delta D$ and the Quantization Error via the $\text{D}^3\text{ASA}$ module
                yields the best performance, validating our core design principles. **(Full table in the paper)**
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <hr>


    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">

              <div class="publication-video">
                <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0"
                  allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Additional Results/Demos</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-video1">
              <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
                <source src="static/videos/carousel1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">Visualization of feature maps across different layers.</h2>
            </div>
            <div class="item item-video2">
              <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
                <source src="static/videos/carousel2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">Demonstration of detection on images with post-processing.</h2>
            </div>
            <div class="item item-video3">
              <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
                <source src="static/videos/carousel3.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">Comparison of D³QE vs. baselines on challenging samples.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <hr>


    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Poster</h2>

          <iframe src="static/pdfs/iccv25_poster_template.pdf" width="100%" height="550">
          </iframe>

        </div>
      </div>
    </section>
    <hr>


    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{Zhang2025D3QE,
  title={D³QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection},
  author={Yanran Zhang and Bingyao Yu and Yu Zheng and Wenzhao Zheng and Yueqi Duan and Lei Chen and Jie Zhou and Jiwen Lu},
  journal={ICCV},
  year={2025},
  url={https://YOUR-DOMAIN.com/YOUR-PROJECT-PAGE},
  code={https://github.com/Zhangyr2022/D3QE}
}</code></pre>
      </div>
    </section>
    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

</body>

</html>